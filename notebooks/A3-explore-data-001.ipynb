{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "from unidecode import unidecode\n",
    "from typing import List, Tuple, Dict, Union\n",
    "import log\n",
    "import mynlputils as nu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(raw_txt_train_path: str, raw_txt_test_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads the training and test data.\n",
    "    Args:\n",
    "        raw_txt_train_path (str): Path to the training data file.\n",
    "        raw_txt_test_path (str): Path to the test data file.\n",
    "    Returns:\n",
    "        train_data (DataFrame): Training data.\n",
    "        test_data (DataFrame): Test data.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(raw_txt_train_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    df_test = pd.read_csv(raw_txt_test_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    logger.info(f\"df_train.shape: {df_train.shape}\")\n",
    "    logger.info(f\"df_train unique Titles: {df_train['title'].nunique()}\")\n",
    "    logger.info(f\"df_train unique Labels: {df_train['label'].value_counts()}\")\n",
    "    logger.info(f\"df_test.shape: {df_test.shape}\")\n",
    "    logger.info(f\"df_test unique Titles: {df_test['title'].nunique()}\")\n",
    "    logger.info(f\"df_test unique Labels: {df_test['label'].value_counts()}\")\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-Jul-23 19:14:29 - INFO - Starting 'load_config'.\n",
      "22-Jul-23 19:14:29 - INFO - Finished 'load_config' in 0.0072 secs.\n"
     ]
    }
   ],
   "source": [
    "conf = nu.load_config(\"a3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-Jul-23 19:14:29 - INFO - df_train.shape: (120000, 3)\n",
      "22-Jul-23 19:14:29 - INFO - df_train unique Titles: 114364\n",
      "22-Jul-23 19:14:29 - INFO - df_train unique Labels: 3    30000\n",
      "4    30000\n",
      "2    30000\n",
      "1    30000\n",
      "Name: label, dtype: int64\n",
      "22-Jul-23 19:14:29 - INFO - df_test.shape: (7600, 3)\n",
      "22-Jul-23 19:14:29 - INFO - df_test unique Titles: 7569\n",
      "22-Jul-23 19:14:29 - INFO - df_test unique Labels: 3    1900\n",
      "4    1900\n",
      "2    1900\n",
      "1    1900\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = load_data(conf.paths.raw_txt_train, conf.paths.raw_txt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(corpus: pd.DataFrame, valid_size: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates a validation set from a list of DataFrames each representing a sentence.\n",
    "\n",
    "    Args:\n",
    "    corpus (pd.DataFrame): List of DataFrames each representing a sentence.\n",
    "    valid_size (float): Proportion of sentences to include in the validation set.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: Training and validation sets.\n",
    "    \"\"\"\n",
    "    train_corpus, valid_corpus = train_test_split(corpus, test_size=valid_size, random_state=1)\n",
    "    return train_corpus, valid_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, valid_corpus = create_validation_set(corpus = df_train, valid_size = 0.063)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: pd.DataFrame, min_freq: int = 5):\n",
    "    \"\"\"\n",
    "    Preprocesses the data by tokenizing the text and replacing low-frequency words with <UNK>.\n",
    "    Args:\n",
    "        data (DataFrame): Input data.\n",
    "        min_freq (int): Minimum frequency for a word to be kept. Default is 5.\n",
    "    Returns:\n",
    "        tokens (List[List[str]]): Tokenized text.\n",
    "        word2idx (dict): A dictionary mapping words to their indices.\n",
    "    \"\"\"\n",
    "    tokens = data['description'].str.lower().str.split().tolist()\n",
    "    word_counts = Counter([word for sentence in tokens for word in sentence])\n",
    "    words = [word if word_counts[word] >= min_freq else '<UNK>' for word in word_counts]\n",
    "    word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "    tokens = [[word if word in word2idx else '<UNK>' for word in sentence] for sentence in tokens]\n",
    "    return tokens, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, word2idx = preprocess_data(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_occurrence_matrix(corpus: list) -> dict:\n",
    "    \"\"\"\n",
    "    Create co-occurrence matrix for the given corpus.\n",
    "    Args:\n",
    "        corpus (list): The corpus of tokenized sentences.\n",
    "    Returns:\n",
    "        co_occurrence_matrix (dict of dicts): The co-occurrence matrix.\n",
    "    \"\"\"\n",
    "    co_occurrence_matrix = Counter()\n",
    "    for sentence in corpus:\n",
    "        # use combinations window of size 2 to find adjacent words\n",
    "        for word1, word2 in combinations(sentence, 2):\n",
    "            co_occurrence_matrix[(word1, word2)] += 1\n",
    "\n",
    "    return co_occurrence_matrix\n",
    "\n",
    "\n",
    "class GloVeModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int):\n",
    "        super(GloVeModel, self).__init__()\n",
    "        self.wi = nn.Embedding(vocab_size, embed_size)\n",
    "        self.wj = nn.Embedding(vocab_size, embed_size)\n",
    "        self.bi = nn.Embedding(vocab_size, 1)\n",
    "        self.bj = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, i_indices: torch.Tensor, j_indices: torch.Tensor):\n",
    "        w_i = self.wi(i_indices)\n",
    "        w_j = self.wj(j_indices)\n",
    "        b_i = self.bi(i_indices).squeeze()\n",
    "        b_j = self.bj(j_indices).squeeze()\n",
    "\n",
    "        x = torch.sum(w_i * w_j, dim=1) + b_i + b_j\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def glove_loss(x_hat: torch.Tensor, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Loss function for GloVe model.\n",
    "    Args:\n",
    "        x_hat (torch.Tensor): Predicted log of co-occurrence.\n",
    "        x (torch.Tensor): True log of co-occurrence.\n",
    "    Returns:\n",
    "        loss (torch.Tensor): The computed loss.\n",
    "    \"\"\"\n",
    "    return torch.mean((x_hat - x) ** 2)\n",
    "\n",
    "\n",
    "def train_glove_model(corpus: list, word2idx: dict, co_occurrence_matrix: dict, embed_size: int = 100, epochs: int = 5):\n",
    "    model = GloVeModel(len(word2idx), embed_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for (word_i, word_j), n_ij in co_occurrence_matrix.items():\n",
    "            i_idx = torch.tensor([word2idx[word_i]], dtype=torch.long)\n",
    "            j_idx = torch.tensor([word2idx[word_j]], dtype=torch.long)\n",
    "            n_ij_tensor = torch.tensor([n_ij], dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(i_idx, j_idx)\n",
    "            loss = glove_loss(outputs, torch.log(n_ij_tensor))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Loss: {total_loss}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix = create_co_occurrence_matrix(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m train_glove_model(tokens, word2idx, co_occurrence_matrix)\n",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb Cell 13\u001b[0m in \u001b[0;36mtrain_glove_model\u001b[0;34m(corpus, word2idx, co_occurrence_matrix, embed_size, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m n_ij_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([n_ij], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(i_idx, j_idx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m loss \u001b[39m=\u001b[39m glove_loss(outputs, torch\u001b[39m.\u001b[39mlog(n_ij_tensor))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlai_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb Cell 13\u001b[0m in \u001b[0;36mGloVeModel.forward\u001b[0;34m(self, i_indices, j_indices)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, i_indices: torch\u001b[39m.\u001b[39mTensor, j_indices: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     w_i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwi(i_indices)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     w_j \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwj(j_indices)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     b_i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbi(i_indices)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X26sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     b_j \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbj(j_indices)\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlai_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlai_nlp/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlai_nlp/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model = train_glove_model(tokens, word2idx, co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-gram model for training word embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_size: int):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.in_embed = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.out_embed = torch.nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, target: torch.Tensor, context: torch.Tensor):\n",
    "        in_embeds = self.in_embed(target)\n",
    "        out_embeds = self.out_embed(context)\n",
    "        scores = torch.matmul(in_embeds, out_embeds.t())\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_embeddings(data: list, word2idx: dict, embed_size: int = 100, epochs: int = 5):\n",
    "    \"\"\"\n",
    "    Trains word embeddings using a Skip-gram model.\n",
    "    Args:\n",
    "        data (List[List[str]]): Tokenized text data.\n",
    "        word2idx (dict): A dictionary mapping words to their indices.\n",
    "        embed_size (int): Size of the word embeddings. Default is 100.\n",
    "        epochs (int): Number of training epochs. Default is 5.\n",
    "    Returns:\n",
    "        model (SkipGramModel): Trained Skip-gram model.\n",
    "    \"\"\"\n",
    "    model = SkipGramModel(len(word2idx), embed_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for sentence in data:\n",
    "            sentence_indices = [word2idx[word] for word in sentence]\n",
    "            for i in range(1, len(sentence_indices) - 1):\n",
    "                context = [sentence_indices[i-1], sentence_indices[i+1]]\n",
    "                target = sentence_indices[i]\n",
    "                target_tensor = torch.tensor([target], dtype=torch.long)\n",
    "                \n",
    "                for c in context:\n",
    "                    context_tensor = torch.tensor([c], dtype=torch.long)\n",
    "                    scores = model(target_tensor, context_tensor)\n",
    "                    loss = loss_fn(scores, context_tensor)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "        logger.info(f'Epoch {epoch + 1}, Loss: {total_loss / len(data)}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m train_word_embeddings(tokens, word2idx)\n",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain_word_embeddings\u001b[0;34m(data, word2idx, embed_size, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m context_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([c], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m scores \u001b[39m=\u001b[39m model(target_tensor, context_tensor)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(scores, context_tensor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-001.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlai_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlai_nlp/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlai_nlp/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "model = train_word_embeddings(tokens, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, word2idx, words):\n",
    "    \"\"\"\n",
    "    Visualizes word embeddings using t-SNE and Plotly.\n",
    "    Args:\n",
    "        model (SkipGramModel): Trained Skip-gram model.\n",
    "        word2idx (dict): A dictionary mapping words to their indices.\n",
    "        words (list): List of words to visualize.\n",
    "    \"\"\"\n",
    "    word_embeds = model.in_embed.weight.data.numpy()\n",
    "    words_idx = [word2idx[word] for word in words]\n",
    "    words_embed = word_embeds[words_idx]\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    words_tsne = tsne.fit_transform(words_embed)\n",
    "\n",
    "    df = pd.DataFrame(words_tsne, columns=['x', 'y'])\n",
    "    df['word'] = words\n",
    "\n",
    "    fig = px.scatter(df, x='x', y='y', text='word')\n",
    "    fig.update_traces(textposition='top center')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
