{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import log\n",
    "import mynlputils as nu\n",
    "logger = log.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_trigrams(text: str) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Function to generate character-level trigrams from text.\n",
    "\n",
    "    Args:\n",
    "    text (str): Text data.\n",
    "\n",
    "    Returns:\n",
    "    List[Tuple[str, str, str]]: List of character-level trigrams.\n",
    "    \"\"\"\n",
    "    return list(ngrams(text, 3, pad_left=True, pad_right=True, left_pad_symbol='<s><s>', right_pad_symbol='</s>'))\n",
    "\n",
    "@nu.timer\n",
    "def load_datasets(base_path_train: str, base_path_val: str, langs: List[str]) -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Function to load the training and validation datasets.\n",
    "\n",
    "    Args:\n",
    "    base_path_train (str): Base path to the training files.\n",
    "    base_path_val (str): Base path to the validation files.\n",
    "    langs (List[str]): List of keys for the dictionaries.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Dict[str, str], Dict[str, str]]: Tuple of two dictionaries containing the training and validation data.\n",
    "    \"\"\"\n",
    "    training_data = {}\n",
    "    validation_data = {}\n",
    "    for lang in langs:\n",
    "        training_file = os.path.join(base_path_train, f\"norm_train.{lang}.txt\")\n",
    "        validation_file = os.path.join(base_path_val, f\"val.{lang}.txt\")\n",
    "        training_data[lang] = nu.load_text_data(training_file)\n",
    "        validation_data[lang] = nu.load_text_data(validation_file)\n",
    "    return training_data, validation_data\n",
    "\n",
    "@nu.timer\n",
    "def load_models(base_path: str, langs: List[str]) -> Dict[str, Dict[Tuple[str, str], Counter]]:\n",
    "    \"\"\"\n",
    "    Function to load the language models.\n",
    "\n",
    "    Args:\n",
    "    base_path (str): Base path to the model files.\n",
    "    langs (List[str]): List of keys for the dictionary.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Dict[Tuple[str, str], Counter]]: Dictionary containing the language models.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for lang in langs:\n",
    "        model_file = os.path.join(base_path, f\"model_{lang}.json\")\n",
    "        models[lang] = nu.load_model(model_file)\n",
    "    return models\n",
    "\n",
    "\n",
    "def generate_text(model: Dict[Tuple[str, str], Counter], max_length: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Function to generate text from a character-level trigram language model.\n",
    "\n",
    "    Args:\n",
    "    model (Dict[Tuple[str, str], Counter]): Language model.\n",
    "    max_length (int, optional): Maximum length of the generated text. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "    str: Generated text.\n",
    "    \"\"\"\n",
    "    text = ['<s>', '<s>']\n",
    "    while len(text) < max_length:\n",
    "        t1, t2 = text[-2], text[-1]\n",
    "        if (t1, t2) in model:\n",
    "            # next_char = model[(t1, t2)].most_common(1)[0][0]\n",
    "            possible_chars = list(model[(t1, t2)].keys())\n",
    "            probabilities = [count / sum(model[(t1, t2)].values()) for count in model[(t1, t2)].values()]\n",
    "            next_char = random.choices(possible_chars, probabilities)[0]\n",
    "        else:\n",
    "            # If the current pair of characters is not in the model,\n",
    "            # append a random character or implement another strategy\n",
    "            next_char = random.choice(string.ascii_lowercase + ' ')\n",
    "        # t1, t2 = text[-2], text[-1]\n",
    "        # next_chars = model[(t1, t2)].most_common(1)\n",
    "        # if next_chars:\n",
    "        #     next_char = next_chars[0][0]\n",
    "        # else:\n",
    "        #     next_char = '<s>'  # Or some other strategy\n",
    "        text.append(next_char)\n",
    "    return ''.join(text)\n",
    "\n",
    "@nu.timer\n",
    "def save_generated_text(generated_text: Dict[str, str], dir_path: str):\n",
    "    \"\"\"\n",
    "    Function to save the generated text to files.\n",
    "\n",
    "    Args:\n",
    "    generated_text (Dict[str, str]): Dictionary containing the generated text, with language names as keys.\n",
    "    dir_path (str): Directory path where the files will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path) # Create directory if it does not exist\n",
    "\n",
    "    for lang, text in generated_text.items():\n",
    "        file_path = os.path.join(dir_path, f'generated_text_{lang}.txt')\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "\n",
    "\n",
    "def calculate_perplexity(model: Dict[Tuple[str, str], Counter], text: str) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the perplexity of a language model on a given text.\n",
    "\n",
    "    Args:\n",
    "    model (Dict[Tuple[str, str], Counter]): Language model.\n",
    "    text (str): Text data.\n",
    "\n",
    "    Returns:\n",
    "    float: Perplexity of the language model on the text.\n",
    "    \"\"\"\n",
    "    trigrams = generate_trigrams(text)\n",
    "    N = len(trigrams)\n",
    "    log_prob = 0\n",
    "    for t1, t2, t3 in trigrams:\n",
    "        total = sum(model[(t1, t2)].values())\n",
    "        if total == 0:\n",
    "            # Choose a small nonzero value if the bigram does not exist in the model\n",
    "            prob = 1e-10\n",
    "        else:\n",
    "            prob = model[(t1, t2)][t3] / total\n",
    "        log_prob += np.log2(prob) if prob > 0 else 0\n",
    "    return np.power(2, -log_prob/N)\n",
    "\n",
    "@nu.timer\n",
    "def generate_perplexity_table(models: Dict[str, Dict[Tuple[str, str], Counter]], \n",
    "                              validation_sets: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to generate a table of perplexity scores for multiple language models\n",
    "    and multiple validation sets.\n",
    "\n",
    "    Args:\n",
    "    models (Dict[str, Dict[Tuple[str, str], Counter]]): Dictionary of language models,\n",
    "        with language names as keys.\n",
    "    validation_sets (Dict[str, str]): Dictionary of validation sets, with language\n",
    "        names as keys.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with perplexity scores, with validation set languages as rows\n",
    "        and model languages as columns.\n",
    "    \"\"\"\n",
    "    perplexity_scores = {}\n",
    "    for model_lang, model in models.items():\n",
    "        scores = {}\n",
    "        for val_lang, val_text in validation_sets.items():\n",
    "            score = calculate_perplexity(model, val_text)\n",
    "            scores[val_lang] = score\n",
    "        perplexity_scores[model_lang] = scores\n",
    "\n",
    "    df = pd.DataFrame(perplexity_scores)\n",
    "    return df\n",
    "\n",
    "@nu.timer\n",
    "def plot_perplexity(df: pd.DataFrame, plot_filepath: str, title: str):\n",
    "    \"\"\"\n",
    "    Function to plot a DataFrame of perplexity scores.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame with perplexity scores, with validation set languages as rows\n",
    "        and model languages as columns.\n",
    "    \"\"\"\n",
    "    # Convert DataFrame from wide format to long format\n",
    "    df_long = df.reset_index().melt(id_vars='index', var_name='Model Language', value_name='Perplexity')\n",
    "\n",
    "    # Create bar chart\n",
    "    fig = px.bar(df_long, x='index', y='Perplexity', color='Model Language', \n",
    "                 labels={'index': 'Validation Set Language'}, barmode='group', title=title)\n",
    "    # Save plot\n",
    "    nu.save_plot(plot_filepath, fig, title)\n",
    "\n",
    "\n",
    "def calculate_accuracy(predicted: List[str], actual: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy of predicted language labels.\n",
    "\n",
    "    Args:\n",
    "    predicted (List[str]): List of predicted language labels.\n",
    "    actual (List[str]): List of actual language labels.\n",
    "\n",
    "    Returns:\n",
    "    float: Accuracy score.\n",
    "    \"\"\"\n",
    "    correct_predictions = sum(p == a for p, a in zip(predicted, actual))\n",
    "    total_predictions = len(predicted)\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "\n",
    "\n",
    "def classify_text(filepath: str, models: Dict[str, Dict[Tuple[str, str], Counter]]) -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Function to classify texts based on language models.\n",
    "\n",
    "    Args:\n",
    "    filepath (str): Filepath to the text file to be classified.\n",
    "    models (Dict[str, Dict[Tuple[str, str], Counter]]): Dictionary of language models.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Dict[str, str], Dict[str, str]]: Tuple of two dictionaries. \n",
    "    The first dictionary contains the classified texts with predicted language labels as keys and texts as values.\n",
    "    The second dictionary contains the classified texts with actual language labels as keys and texts as values.\n",
    "    \"\"\"\n",
    "    predicted_texts = []\n",
    "    actual_texts = []\n",
    "    texts = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            actual_lang = line[:2]\n",
    "            text = line[3:].strip()\n",
    "            perplexities = {lang: calculate_perplexity(model, text) for lang, model in models.items()}\n",
    "            predicted_lang = min(perplexities, key=perplexities.get)\n",
    "            predicted_texts.append(predicted_lang)\n",
    "            actual_texts.append(actual_lang)\n",
    "            texts.append(text)\n",
    "    return predicted_texts, actual_texts, texts\n",
    "\n",
    "\n",
    "def get_confusion_matrix(actual, predicted, labels):\n",
    "    \"\"\"\n",
    "    Function to compute the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    actual (list): List of actual labels.\n",
    "    predicted (list): List of predicted labels.\n",
    "    labels (list): List of unique labels.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The confusion matrix.\n",
    "    \"\"\"\n",
    "    return confusion_matrix(actual, predicted, labels=labels)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, labels, filepath):\n",
    "    \"\"\"\n",
    "    Function to plot the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (np.array): The confusion matrix.\n",
    "    labels (list): List of unique labels.\n",
    "    filepath (str): Filepath to save the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(cm, index=labels, columns=labels).T\n",
    "    fig = px.imshow(df_cm, labels=dict(x=\"True Label\", y=\"Predicted Label\", color=\"Count\"), \n",
    "                     x=labels, y=labels, title=\"Confusion Matrix\")\n",
    "    fig.write_html(filepath)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    conf = nu.load_config(\"a1\") # Load config\n",
    "    train_data, valid_data = load_datasets(conf.paths.normalized_txt, conf.paths.raw_txt, conf.langs) # load data\n",
    "    models = load_models(conf.paths.models, conf.langs) # load models\n",
    "    # Generate text\n",
    "    generated_text = {}\n",
    "    for lang in conf.langs:\n",
    "        generated_text[lang] = generate_text(models[lang])\n",
    "\n",
    "    # Save generated text\n",
    "    save_generated_text(generated_text, conf.paths.gen_text)\n",
    "    # Generate perplexity table\n",
    "    perplexity_table_train = generate_perplexity_table(models, train_data)\n",
    "    perplexity_table_valid = generate_perplexity_table(models, valid_data)\n",
    "    # Plot perplexity\n",
    "    plot_perplexity(perplexity_table_train, conf.paths.reporting_plots, 'Perplexity of trigram language models on training sets')\n",
    "    plot_perplexity(perplexity_table_valid, conf.paths.reporting_plots, 'Perplexity of trigram language models on validation sets')\n",
    "\n",
    "    predicted_texts, actual_texts = classify_text(f\"{conf.paths.raw_txt}test.lid.txt\", models)\n",
    "    accuracy = calculate_accuracy(predicted_texts, actual_texts)\n",
    "    # Calculate confusion matrix\n",
    "    cm = get_confusion_matrix(list(actual_texts.keys()), list(predicted_texts.keys()), list(models.keys()))\n",
    "    plot_confusion_matrix(cm, list(models.keys()), f'{conf.paths.reporting_plots}confusion_matrix.html') # Plot confusion matrix\n",
    "    logger.info(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Jul-23 16:10:51 - INFO - Starting 'load_config'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_config' in 0.0069 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_datasets'.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0008 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0004 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0011 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0004 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0006 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0007 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0006 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0003 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0005 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_text_data'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_text_data' in 0.0005 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_datasets' in 0.0103 secs.\n",
      "10-Jul-23 16:10:51 - INFO - Starting 'load_models'.\n",
      "10-Jul-23 16:10:51 - INFO - Finished 'load_models' in 0.0283 secs.\n"
     ]
    }
   ],
   "source": [
    "conf = nu.load_config(\"a1\") # Load config\n",
    "train_data, valid_data = load_datasets(conf.paths.normalized_txt, conf.paths.raw_txt, conf.langs) # load data\n",
    "models = load_models(conf.paths.models, conf.langs) # load models\n",
    "# Generate text\n",
    "generated_text = {}\n",
    "for lang in conf.langs:\n",
    "    generated_text[lang] = generate_text(models[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_texts, actual_texts = classify_text(f\"{conf.paths.raw_txt}test.lid.txt\", models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{conf.paths.raw_txt}test.lid.txt\", 'r') as file:\n",
    "#     for line in file:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = nu.load_text_data(f\"{conf.paths.raw_txt}test.lid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in test_data:\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Jul-23 16:10:54 - INFO - Accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "predicted_texts, actual_texts, texts = classify_text(f\"{conf.paths.raw_txt}test.lid.txt\", models)\n",
    "accuracy = calculate_accuracy(predicted_texts, actual_texts)\n",
    "# Calculate confusion matrix\n",
    "cm = get_confusion_matrix(actual_texts, predicted_texts, list(models.keys()))\n",
    "plot_confusion_matrix(cm, list(models.keys()), f'{conf.paths.reporting_plots}confusion_matrix.html') # Plot confusion matrix\n",
    "logger.info(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_vocab(text):\n",
    "    words = text.split()\n",
    "    vocab = Counter(words)\n",
    "    return {' '.join(word): freq for word, freq in vocab.items()}\n",
    "\n",
    "def byte_pair_encoding(text, num_iterations):\n",
    "    vocab = get_vocab(text)\n",
    "    for i in range(num_iterations):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 1, 'newer': 1, 'wider': 1, 'vi ewer': 1}\n"
     ]
    }
   ],
   "source": [
    "# test BPE\n",
    "text = 'lower newer wider viewer'  # example text\n",
    "num_iterations = 10\n",
    "vocab = byte_pair_encoding(text, num_iterations)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
