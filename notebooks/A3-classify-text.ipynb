{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import string\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import log\n",
    "import mynlputils as nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nu.timer\n",
    "def load_data(raw_txt_train_path: str, raw_txt_test_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_train = pd.read_csv(raw_txt_train_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    df_test = pd.read_csv(raw_txt_test_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    return df_train[[\"label\", \"description\"]], df_test[[\"label\", \"description\"]]\n",
    "\n",
    "@nu.timer\n",
    "def create_validation_set(corpus: pd.DataFrame, valid_size: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_corpus, valid_corpus = train_test_split(corpus, test_size=valid_size, random_state=1)\n",
    "    return train_corpus.reset_index(drop=True), valid_corpus.reset_index(drop=True)\n",
    "\n",
    "@nu.timer\n",
    "def clean_text(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    clean_docs = docs['description']\n",
    "    clean_docs = clean_docs.str.replace(\"-\", \" \")\n",
    "    clean_docs = clean_docs.str.replace(\"quot;\", \" \")\n",
    "    clean_docs = clean_docs.str.replace(\"#39;s\", \"'\")\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "    clean_docs = clean_docs.str.translate(translation_table)\n",
    "    clean_docs = clean_docs.str.lower()\n",
    "    clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ')\n",
    "    clean_docs = clean_docs.str.replace(r'\\s+', ' ')\n",
    "    return clean_docs.to_frame()\n",
    "\n",
    "@nu.timer\n",
    "def split_docs(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    return docs['description'].str.split().to_list()\n",
    "\n",
    "@nu.timer\n",
    "def tokenize(tokens: List[List[str]], min_freq: int = 5):\n",
    "    word_freq = Counter([word for sentence in tokens for word in sentence])\n",
    "    vocab = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
    "    vocab = ['<PAD>', '<UNK>'] + vocab\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_tokens = [[word2idx.get(word, 1) for word in sentence] for sentence in tokens]\n",
    "    return vocab, idx_tokens, word2idx\n",
    "\n",
    "@nu.timer\n",
    "def create_skipgrams(corpus, window_size, pad_idx):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        padded_sentence = [pad_idx] * window_size + sentence + [pad_idx] * window_size\n",
    "        for word_index in range(window_size, len(padded_sentence) - window_size):\n",
    "            contexts = padded_sentence[word_index - window_size : word_index] + padded_sentence[word_index + 1 : word_index + window_size + 1]\n",
    "            data.append((contexts, padded_sentence[word_index]))\n",
    "    return data\n",
    "\n",
    "@nu.timer\n",
    "def create_batches(skipgrams, word2idx, pad_idx, batch_size=32, num_neg_samples=5):\n",
    "    words_list = list(word2idx.keys())\n",
    "    vocab_size = len(words_list)\n",
    "    n = len(skipgrams)\n",
    "\n",
    "    # Shuffle skipgrams\n",
    "    random.shuffle(skipgrams)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for batch_start in range(0, n, batch_size):\n",
    "        context_batch = []\n",
    "        target_batch = []\n",
    "        negative_batch = []\n",
    "\n",
    "        # Create batches\n",
    "        for contexts, target in skipgrams[batch_start:batch_start + batch_size]:\n",
    "            negatives = [random.choice(range(vocab_size)) for _ in range(num_neg_samples)]\n",
    "            context_batch.append(torch.LongTensor(contexts))\n",
    "            target_batch.append(torch.LongTensor([target]))\n",
    "            negative_batch.append(torch.LongTensor(negatives))\n",
    "\n",
    "        # If this is the last batch and it's not full, skip it\n",
    "        if len(context_batch) < batch_size:\n",
    "            continue\n",
    "\n",
    "        # Pad context sequences in batch\n",
    "        context_batch = pad_sequence(context_batch, batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "        # Convert target and negative batches to tensors\n",
    "        target_batch = torch.stack(target_batch)\n",
    "        negative_batch = torch.stack(negative_batch)\n",
    "\n",
    "        batches.append((context_batch, target_batch, negative_batch))\n",
    "        \n",
    "    return batches\n",
    "\n",
    "\n",
    "class CBOW_NS(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW_NS, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, context_words, target_word, negative_words):\n",
    "        # Get embeddings for context words, target word and negative words\n",
    "        context_embeds = self.embeddings(context_words)  # (batch_size, window_size*2, embed_size)\n",
    "        target_embeds = self.embeddings(target_word)    # (batch_size, 1, embed_size)\n",
    "        negative_embeds = self.embeddings(negative_words)# (batch_size, num_neg_samples, embed_size)\n",
    "\n",
    "        # Sum the context word embeddings\n",
    "        context_embeds_sum = torch.sum(context_embeds, dim=1, keepdim=True)  # (batch_size, 1, embed_size)\n",
    "\n",
    "        # Compute positive score\n",
    "        pos_score = torch.bmm(context_embeds_sum, target_embeds.transpose(1,2)) # (batch_size, 1, 1)\n",
    "        pos_score = F.logsigmoid(pos_score)\n",
    "\n",
    "        # Compute negative score\n",
    "        neg_score = torch.bmm(context_embeds_sum, negative_embeds.transpose(1,2)) # (batch_size, 1, num_neg_samples)\n",
    "        neg_score = F.logsigmoid(-neg_score)\n",
    "\n",
    "        # Return negative of total score\n",
    "        return -(torch.sum(pos_score) + torch.sum(neg_score))\n",
    "    \n",
    "@nu.timer\n",
    "def train(model, epochs, train_batches, val_batches, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for context_batch, target_batch, negative_batch in train_batches:\n",
    "            model.zero_grad()\n",
    "            loss = model(context_batch, target_batch, negative_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        train_loss = total_loss / batch_count\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()  # set model to eval mode\n",
    "        val_loss = evaluate(model, val_batches)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        logger.info(f'Epoch {epoch}, Train Loss: {train_losses[-1]}, Validation Loss: {val_loss[-1]}')\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate(model, batches):\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    with torch.no_grad():  # disable gradient computation to save memory\n",
    "        for context_batch, target_batch, negative_batch in batches:\n",
    "            loss = model(context_batch, target_batch, negative_batch)\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "    return total_loss / batch_count\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses, epochs):\n",
    "    epochs_range = list(range(1, epochs + 1))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=epochs_range, y=train_losses, mode='lines', name='Train Loss'))\n",
    "    fig.add_trace(go.Scatter(x=epochs_range, y=val_losses, mode='lines', name='Validation Loss'))\n",
    "    fig.update_layout(title='Loss over Epochs', xaxis=dict(title='Epoch'), yaxis=dict(title='Loss'))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-Jul-23 19:52:59 - INFO - Starting 'load_config'.\n",
      "24-Jul-23 19:52:59 - INFO - Finished 'load_config' in 0.0256 secs.\n"
     ]
    }
   ],
   "source": [
    "conf = nu.load_config(\"a3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-Jul-23 19:30:06 - INFO - Starting 'load_config'.\n",
      "24-Jul-23 19:30:06 - INFO - Finished 'load_config' in 0.0196 secs.\n",
      "24-Jul-23 19:30:06 - INFO - Starting 'load_data'.\n",
      "24-Jul-23 19:30:06 - INFO - Finished 'load_data' in 0.3146 secs.\n",
      "24-Jul-23 19:30:06 - INFO - Starting 'create_validation_set'.\n",
      "24-Jul-23 19:30:06 - INFO - Finished 'create_validation_set' in 0.0076 secs.\n",
      "24-Jul-23 19:30:06 - INFO - Starting 'clean_text'.\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_25296/2887024429.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ')\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_25296/2887024429.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\s+', ' ')\n",
      "24-Jul-23 19:30:07 - INFO - Finished 'clean_text' in 1.4201 secs.\n",
      "24-Jul-23 19:30:07 - INFO - Starting 'clean_text'.\n",
      "24-Jul-23 19:30:08 - INFO - Finished 'clean_text' in 0.1519 secs.\n",
      "24-Jul-23 19:30:08 - INFO - Starting 'clean_text'.\n",
      "24-Jul-23 19:30:08 - INFO - Finished 'clean_text' in 0.1036 secs.\n",
      "24-Jul-23 19:30:08 - INFO - Starting 'split_docs'.\n",
      "24-Jul-23 19:30:08 - INFO - Finished 'split_docs' in 0.2672 secs.\n",
      "24-Jul-23 19:30:08 - INFO - Starting 'split_docs'.\n",
      "24-Jul-23 19:30:08 - INFO - Finished 'split_docs' in 0.0250 secs.\n",
      "24-Jul-23 19:30:08 - INFO - Starting 'split_docs'.\n",
      "24-Jul-23 19:30:08 - INFO - Finished 'split_docs' in 0.0126 secs.\n",
      "24-Jul-23 19:30:08 - INFO - Starting 'tokenize'.\n",
      "24-Jul-23 19:30:09 - INFO - Finished 'tokenize' in 0.7863 secs.\n",
      "24-Jul-23 19:30:09 - INFO - Starting 'tokenize'.\n",
      "24-Jul-23 19:30:09 - INFO - Finished 'tokenize' in 0.0677 secs.\n",
      "24-Jul-23 19:30:09 - INFO - Starting 'tokenize'.\n",
      "24-Jul-23 19:30:09 - INFO - Finished 'tokenize' in 0.0399 secs.\n",
      "24-Jul-23 19:30:09 - INFO - Starting 'create_skipgrams'.\n",
      "24-Jul-23 19:30:12 - INFO - Finished 'create_skipgrams' in 3.5077 secs.\n",
      "24-Jul-23 19:30:12 - INFO - Starting 'create_skipgrams'.\n",
      "24-Jul-23 19:30:13 - INFO - Finished 'create_skipgrams' in 0.5400 secs.\n",
      "24-Jul-23 19:30:13 - INFO - Starting 'create_skipgrams'.\n",
      "24-Jul-23 19:30:13 - INFO - Finished 'create_skipgrams' in 0.0665 secs.\n",
      "24-Jul-23 19:30:13 - INFO - Starting 'create_batches'.\n",
      "24-Jul-23 19:30:43 - INFO - Finished 'create_batches' in 30.4671 secs.\n",
      "24-Jul-23 19:30:43 - INFO - Starting 'create_batches'.\n",
      "24-Jul-23 19:30:47 - INFO - Finished 'create_batches' in 3.2806 secs.\n",
      "24-Jul-23 19:30:47 - INFO - Starting 'create_batches'.\n",
      "24-Jul-23 19:30:49 - INFO - Finished 'create_batches' in 2.1091 secs.\n"
     ]
    }
   ],
   "source": [
    "# conf = nu.load_config(\"a3\")\n",
    "df_train, df_test = load_data(conf.paths.raw_txt_train, conf.paths.raw_txt_test)\n",
    "df_train, df_valid = create_validation_set(df_train, 0.1)\n",
    "df_train_clean = clean_text(df_train)\n",
    "df_valid_clean = clean_text(df_valid)\n",
    "df_test_clean = clean_text(df_test)\n",
    "\n",
    "train_tokens = split_docs(df_train_clean)\n",
    "valid_tokens = split_docs(df_valid_clean)\n",
    "test_tokens = split_docs(df_test_clean)\n",
    "\n",
    "vocab, idx_train_tokens, word2idx = tokenize(train_tokens)\n",
    "_, idx_valid_tokens, _ = tokenize(valid_tokens)\n",
    "_, idx_test_tokens, _ = tokenize(test_tokens)\n",
    "\n",
    "pad_idx = word2idx['<PAD>']\n",
    "skipgrams_train = create_skipgrams(idx_train_tokens, window_size=2, pad_idx=pad_idx)\n",
    "skipgrams_valid = create_skipgrams(idx_valid_tokens, window_size=2, pad_idx=pad_idx)\n",
    "skipgrams_test = create_skipgrams(idx_test_tokens, window_size=2, pad_idx=pad_idx)\n",
    "\n",
    "train_batches = create_batches(skipgrams_train, word2idx, pad_idx, batch_size=512)\n",
    "valid_batches = create_batches(skipgrams_valid, word2idx, pad_idx, batch_size=512)\n",
    "test_batches = create_batches(skipgrams_test, word2idx, pad_idx, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-Jul-23 19:32:17 - INFO - Starting 'train'.\n",
      "24-Jul-23 19:33:21 - INFO - Epoch 0, Train Loss: 11005.139324033504, Validation Loss: 12427.67182134272\n",
      "24-Jul-23 19:34:26 - INFO - Epoch 1, Train Loss: 4304.808127507672, Validation Loss: 9142.847182724501\n",
      "24-Jul-23 19:35:29 - INFO - Epoch 2, Train Loss: 2716.1542937292415, Validation Loss: 6758.749054290436\n",
      "24-Jul-23 19:36:31 - INFO - Epoch 3, Train Loss: 1850.9295128452666, Validation Loss: 5199.562077448918\n",
      "24-Jul-23 19:37:37 - INFO - Epoch 4, Train Loss: 1382.2247705811515, Validation Loss: 4273.301761501438\n",
      "24-Jul-23 19:38:41 - INFO - Epoch 5, Train Loss: 1126.8104174800965, Validation Loss: 3729.056437062693\n",
      "24-Jul-23 19:39:44 - INFO - Epoch 6, Train Loss: 982.3881940734063, Validation Loss: 3406.6263769799534\n",
      "24-Jul-23 19:40:47 - INFO - Epoch 7, Train Loss: 895.8516813314844, Validation Loss: 3218.291761796553\n",
      "24-Jul-23 19:41:52 - INFO - Epoch 8, Train Loss: 840.1050793181545, Validation Loss: 3113.002150986221\n",
      "24-Jul-23 19:42:56 - INFO - Epoch 9, Train Loss: 801.5078498954342, Validation Loss: 3058.09329860813\n",
      "24-Jul-23 19:42:56 - INFO - Finished 'train' in 639.5655 secs.\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "model = CBOW_NS(vocab_size, conf.model.embedding.embed_size)\n",
    "trained_model, train_losses, val_losses = train(model, conf.model.embedding.epochs, train_batches, valid_batches, conf.model.embedding.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Train Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "y": [
          11005.139324033504,
          4304.808127507672,
          2716.1542937292415,
          1850.9295128452666,
          1382.2247705811515,
          1126.8104174800965,
          982.3881940734063,
          895.8516813314844,
          840.1050793181545,
          801.5078498954342
         ]
        },
        {
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "y": [
          12427.67182134272,
          9142.847182724501,
          6758.749054290436,
          5199.562077448918,
          4273.301761501438,
          3729.056437062693,
          3406.6263769799534,
          3218.291761796553,
          3113.002150986221,
          3058.09329860813
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Loss over Epochs"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(train_losses, val_losses, conf.model.embedding.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embeddings.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {value: key for key, value in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)  # reduce to 2 components\n",
    "embeddings_pca = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot some words\n",
    "words = ['china', 'beijing', 'russia', 'moscow', 'japan', 'tokyo', 'france', 'paris', 'germany', 'berlin']\n",
    "for word in words:\n",
    "    idx = word2idx[word]\n",
    "    plt.scatter(embeddings_pca[idx, 0], embeddings_pca[idx, 1])\n",
    "    plt.annotate(word, (embeddings_pca[idx, 0], embeddings_pca[idx, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding, word, word2idx, idx2word, n=10):\n",
    "    # Get the embedding for the word\n",
    "    word_embedding = embedding[word2idx[word]]\n",
    "\n",
    "    # Compute cosine similarities between word_embedding and all embeddings\n",
    "    similarities = cosine_similarity([word_embedding], embedding)[0]\n",
    "\n",
    "    # Get the indices of the top n similar embeddings\n",
    "    closest_idxs = np.argsort(similarities)[-n:]\n",
    "\n",
    "    # Convert these indices back to words and return\n",
    "    closest_words_with_distances = [(idx2word[idx], similarities[idx]) for idx in reversed(closest_idxs)]\n",
    "\n",
    "    return closest_words_with_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = [\"referendum\", \"venezuela\", \"war\", \"pope\", \"schumacher\", \"ferrari\", \"soccer\", \"cricket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words to 'referendum':\n",
      "referendum: 1.0000001192092896\n",
      "kamal: 0.40073469281196594\n",
      "recall: 0.39591097831726074\n",
      "veil: 0.34378424286842346\n",
      "marking: 0.3431364595890045\n",
      "\n",
      "Closest words to 'venezuela':\n",
      "venezuela: 1.0\n",
      "roy: 0.4125961363315582\n",
      "unsuccessful: 0.402940571308136\n",
      "forcibly: 0.37566354870796204\n",
      "moffett: 0.3735027611255646\n",
      "\n",
      "Closest words to 'war':\n",
      "war: 1.0\n",
      "crimes: 0.5158342123031616\n",
      "civil: 0.48663195967674255\n",
      "terror: 0.4587789475917816\n",
      "iraqi: 0.4506089687347412\n",
      "\n",
      "Closest words to 'pope':\n",
      "pope: 0.9999998807907104\n",
      "phishers: 0.40673336386680603\n",
      "nagpur: 0.38793933391571045\n",
      "pixar: 0.37529370188713074\n",
      "zoellick: 0.3609708547592163\n",
      "\n",
      "Closest words to 'schumacher':\n",
      "schumacher: 1.0000001192092896\n",
      "michael: 0.5231149792671204\n",
      "kanpur: 0.42215320467948914\n",
      "peacekeeping: 0.42100369930267334\n",
      "overwhelming: 0.3956893980503082\n",
      "\n",
      "Closest words to 'ferrari':\n",
      "ferrari: 1.0\n",
      "keeper: 0.4152476191520691\n",
      "steele: 0.39676085114479065\n",
      "philip: 0.37690484523773193\n",
      "cr: 0.35654178261756897\n",
      "\n",
      "Closest words to 'soccer':\n",
      "soccer: 1.0\n",
      "tennis: 0.4730041027069092\n",
      "football: 0.4708947241306305\n",
      "english: 0.46905550360679626\n",
      "womens: 0.4365655183792114\n",
      "\n",
      "Closest words to 'cricket':\n",
      "cricket: 0.9999998807907104\n",
      "match: 0.44146963953971863\n",
      "pro: 0.4271118640899658\n",
      "test: 0.42213869094848633\n",
      "league: 0.4186098873615265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in sample_words:\n",
    "    print(f\"Closest words to '{word}':\")\n",
    "    closest_words_with_distances = find_closest_embeddings(embeddings, word, word2idx, idx2word, n=5)\n",
    "    for close_word, distance in closest_words_with_distances:\n",
    "        print(f\"{close_word}: {distance}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {word: embeddings[i] for i, word in enumerate(vocab)} # Create a dictionary {word: embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_embeddings(text: str, embeddings: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Calculates the average embeddings of a sentence.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The sentence to calculate embeddings for.\n",
    "        embeddings (dict): The word embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: The average embedding of the sentence.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return np.mean([embeddings.get(word, np.zeros((100,))) for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches_classifier(data: List[Tuple[List[np.ndarray], int]], batch_size: int, pad_value: np.ndarray) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Creates batches of text and label pairs for model training.\n",
    "\n",
    "    Parameters:\n",
    "    data (List[Tuple[List[np.ndarray], int]]): List of (text, label) pairs where text is a list of word embeddings and label is an integer.\n",
    "    batch_size (int): The size of each batch.\n",
    "    pad_value (np.ndarray): The value to use for padding.\n",
    "\n",
    "    Returns:\n",
    "    List[Tuple[torch.Tensor, torch.Tensor]]: List of batches.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "\n",
    "    # Shuffle data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for batch_start in range(0, n, batch_size):\n",
    "        text_batch = []\n",
    "        label_batch = []\n",
    "\n",
    "        # Create batches\n",
    "        for text, label in data[batch_start:batch_start + batch_size]:\n",
    "            text_batch.append(torch.FloatTensor(text))\n",
    "            label_batch.append(torch.LongTensor([label]))\n",
    "\n",
    "        # If this is the last batch and it's not full, skip it\n",
    "        if len(text_batch) < batch_size:\n",
    "            continue\n",
    "\n",
    "        # Pad text sequences in batch\n",
    "        text_batch = pad_sequence(text_batch, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "        # Convert label batch to tensor\n",
    "        label_batch = torch.cat(label_batch)\n",
    "\n",
    "        batches.append((text_batch, label_batch))\n",
    "        \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean['embeddings'] = df_train_clean['description'].apply(lambda x: average_embeddings(x, embedding_dict))\n",
    "df_valid_clean['embeddings'] = df_valid_clean['description'].apply(lambda x: average_embeddings(x, embedding_dict))\n",
    "df_train_clean['class'] = df_train['label']\n",
    "df_valid_clean['class'] = df_valid['label']\n",
    "encoded_train_data = list(zip(df_train_clean['embeddings'], df_train_clean['class']))\n",
    "encoded_valid_data = list(zip(df_valid_clean['embeddings'], df_valid_clean['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = create_batches_classifier(encoded_train_data, batch_size=conf.model.classifier.batch_size, pad_value=0)\n",
    "valid_batches = create_batches_classifier(encoded_valid_data, batch_size=conf.model.classifier.batch_size, pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model: nn.Module, \n",
    "          epochs: int, \n",
    "          train_batches: List[Tuple[torch.Tensor, torch.Tensor]], \n",
    "          val_batches: List[Tuple[torch.Tensor, torch.Tensor]], \n",
    "          lr: float, \n",
    "          device: torch.device) -> Tuple[nn.Module, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Trains the model for a specified number of epochs.\n",
    "\n",
    "    Parameters:\n",
    "    model (nn.Module): The model to train.\n",
    "    epochs (int): The number of epochs to train for.\n",
    "    train_batches (List[Tuple[torch.Tensor, torch.Tensor]]): Batches for the training data.\n",
    "    val_batches (List[Tuple[torch.Tensor, torch.Tensor]]): Batches for the validation data.\n",
    "    lr (float): The learning rate for the Adam optimizer.\n",
    "    device (torch.device): The device to train on.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[nn.Module, List[float], List[float]]: The trained model and lists of training and validation losses over epochs.\n",
    "    \"\"\"\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in train_batches:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_batches))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_batches:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "        val_losses.append(total_loss / len(val_batches))\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_losses[-1]}, Validation Loss: {val_losses[-1]}')\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'target_word' and 'negative_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-classify-text.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-classify-text.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model, train_losses, val_losses \u001b[39m=\u001b[39m train_classifier(model, conf\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mclassifier\u001b[39m.\u001b[39;49mepochs, train_batches, valid_batches, conf\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mclassifier\u001b[39m.\u001b[39;49mlr, torch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-classify-text.ipynb Cell 21\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(model, epochs, train_batches, val_batches, lr, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-classify-text.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-classify-text.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-classify-text.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-classify-text.ipynb#X25sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-classify-text.ipynb#X25sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlai_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'target_word' and 'negative_words'"
     ]
    }
   ],
   "source": [
    "model, train_losses, val_losses = train_classifier(model, conf.model.classifier.epochs, train_batches, valid_batches, conf.model.classifier.lr, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
