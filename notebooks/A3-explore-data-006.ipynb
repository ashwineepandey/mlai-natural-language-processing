{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import string\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "import log\n",
    "import mynlputils as nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(raw_txt_train_path: str, raw_txt_test_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_train = pd.read_csv(raw_txt_train_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    df_test = pd.read_csv(raw_txt_test_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    return df_train[[\"label\", \"description\"]], df_test[[\"label\", \"description\"]]\n",
    "\n",
    "\n",
    "def create_validation_set(corpus: pd.DataFrame, valid_size: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_corpus, valid_corpus = train_test_split(corpus, test_size=valid_size, random_state=1)\n",
    "    return train_corpus.reset_index(drop=True), valid_corpus.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def clean_text(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    clean_docs = docs['description']\n",
    "    clean_docs = clean_docs.str.replace(\"-\", \" \")\n",
    "    clean_docs = clean_docs.str.replace(\"quot;\", \" \")\n",
    "    clean_docs = clean_docs.str.replace(\"#39;s\", \"'\")\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "    clean_docs = clean_docs.str.translate(translation_table)\n",
    "    clean_docs = clean_docs.str.lower()\n",
    "    clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ')\n",
    "    clean_docs = clean_docs.str.replace(r'\\s+', ' ')\n",
    "    return clean_docs.to_frame()\n",
    "\n",
    "\n",
    "def split_docs(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    return docs['description'].str.split().to_list()\n",
    "\n",
    "\n",
    "def tokenize(tokens: List[List[str]], min_freq: int = 5):\n",
    "    word_freq = Counter([word for sentence in tokens for word in sentence])\n",
    "    vocab = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
    "    vocab = ['<PAD>', '<UNK>'] + vocab\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_tokens = [[word2idx.get(word, 1) for word in sentence] for sentence in tokens]\n",
    "    return vocab, idx_tokens, word2idx\n",
    "\n",
    "\n",
    "def create_skipgrams(corpus, window_size, pad_idx):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        padded_sentence = [pad_idx] * window_size + sentence + [pad_idx] * window_size\n",
    "        for word_index in range(window_size, len(padded_sentence) - window_size):\n",
    "            contexts = padded_sentence[word_index - window_size : word_index] + padded_sentence[word_index + 1 : word_index + window_size + 1]\n",
    "            data.append((contexts, padded_sentence[word_index]))\n",
    "    return data\n",
    "\n",
    "\n",
    "# def data_generator(skipgrams, word2idx, num_neg_samples=5):\n",
    "#     words_list = list(word2idx.keys())\n",
    "#     vocab_size = len(words_list)\n",
    "\n",
    "#     for contexts, target in skipgrams:\n",
    "#         negatives = [random.choice(range(vocab_size)) for _ in range(num_neg_samples)]\n",
    "#         yield torch.LongTensor(contexts), torch.LongTensor([target]), torch.LongTensor(negatives)\n",
    "\n",
    "\n",
    "def data_generator(skipgrams, word2idx, pad_idx, batch_size=32, num_neg_samples=5):\n",
    "    words_list = list(word2idx.keys())\n",
    "    vocab_size = len(words_list)\n",
    "    n = len(skipgrams)\n",
    "\n",
    "    # Shuffle skipgrams\n",
    "    random.shuffle(skipgrams)\n",
    "\n",
    "    for batch_start in range(0, n, batch_size):\n",
    "        context_batch = []\n",
    "        target_batch = []\n",
    "        negative_batch = []\n",
    "        \n",
    "        # Create batches\n",
    "        for contexts, target in skipgrams[batch_start:batch_start + batch_size]:\n",
    "            negatives = [random.choice(range(vocab_size)) for _ in range(num_neg_samples)]\n",
    "            context_batch.append(torch.LongTensor(contexts))\n",
    "            target_batch.append(torch.LongTensor([target]))\n",
    "            negative_batch.append(torch.LongTensor(negatives))\n",
    "\n",
    "        # Pad context sequences in batch\n",
    "        context_batch = pad_sequence(context_batch, batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "        # Convert target and negative batches to tensors\n",
    "        target_batch = torch.stack(target_batch)\n",
    "        negative_batch = torch.stack(negative_batch)\n",
    "        \n",
    "        yield context_batch, target_batch, negative_batch\n",
    "\n",
    "# class CBOW_NS(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_size):\n",
    "#         super(CBOW_NS, self).__init__()\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "#     def forward(self, context_words, target_word, negative_words):\n",
    "#         # Get embeddings for context words\n",
    "#         context_embeds = self.embeddings(context_words) # shape: (batch_size, window_size * 2, embed_size)\n",
    "        \n",
    "#         # Get embeddings for target word\n",
    "#         target_embeds = self.embeddings(target_word) # shape: (batch_size, 1, embed_size)\n",
    "#         target_embeds = torch.transpose(target_embeds, 1, 2)  # shape: (batch_size, embed_size, 1)\n",
    "        \n",
    "#         # Get embeddings for negative samples\n",
    "#         negative_embeds = self.embeddings(negative_words).unsqueeze(3) # shape: (batch_size, num_neg_samples, embed_size, 1)\n",
    "#         negative_embeds = negative_embeds.permute(0, 2, 1, 3).squeeze(-1) # shape: (batch_size, embed_size, num_neg_samples)\n",
    "\n",
    "#         # Compute positive score\n",
    "#         pos_score = torch.bmm(context_embeds, target_embeds) # shape: (batch_size, window_size * 2, 1)\n",
    "#         pos_score = F.logsigmoid(pos_score).sum(1) # Sum scores across context words for each target word in the batch\n",
    "\n",
    "#         # Compute negative score\n",
    "#         neg_score = torch.bmm(context_embeds.unsqueeze(2), negative_embeds) # shape: (batch_size, window_size * 2, num_neg_samples)\n",
    "#         neg_score = F.logsigmoid(-neg_score).sum(1) # Sum scores across context words and negative samples for each target word in the batch\n",
    "\n",
    "#         # Return negative of total score\n",
    "#         return -(pos_score + neg_score).mean() # Average across the batch\n",
    "\n",
    "\n",
    "# def train(model, epochs, data_generator, lr=0.001):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "#         for context, target, negative in data_generator:\n",
    "#             model.zero_grad()\n",
    "#             loss = model(context, target, negative)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#         print(f'Loss at epoch {epoch}: {total_loss}')\n",
    "\n",
    "# def train(model, epochs, data_generator, lr=0.001):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "#         for context_batch, target_batch, negative_batch in data_generator:\n",
    "#             model.zero_grad()\n",
    "#             loss = model(context_batch, target_batch, negative_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#         print(f'Loss at epoch {epoch}: {total_loss}')\n",
    "\n",
    "# def train(model, epochs, data_generator, batch_size, lr=0.001):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     data_size = len(data_generator)\n",
    "#     num_batches = (data_size + batch_size - 1) // batch_size\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "#         for batch_idx in range(num_batches):\n",
    "#             start_idx = batch_idx * batch_size\n",
    "#             end_idx = min((batch_idx + 1) * batch_size, data_size)\n",
    "#             batch = data_generator[start_idx:end_idx]\n",
    "#             context, target, negative = zip(*batch)\n",
    "#             context = torch.stack(context)\n",
    "#             target = torch.stack(target)\n",
    "#             negative = torch.stack(negative)\n",
    "#             model.zero_grad()\n",
    "#             loss = model(context, target, negative)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#         print(f'Loss at epoch {epoch}: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-Jul-23 13:32:41 - INFO - Starting 'load_config'.\n",
      "23-Jul-23 13:32:41 - INFO - Finished 'load_config' in 0.0071 secs.\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_83183/3974071394.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ')\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_83183/3974071394.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\s+', ' ')\n"
     ]
    }
   ],
   "source": [
    "conf = nu.load_config(\"a3\")\n",
    "df_train, df_test = load_data(conf.paths.raw_txt_train, conf.paths.raw_txt_test)\n",
    "df_train, df_valid = create_validation_set(df_train, 0.1)\n",
    "df_train_clean = clean_text(df_train)\n",
    "df_valid_clean = clean_text(df_valid)\n",
    "df_test_clean = clean_text(df_test)\n",
    "\n",
    "train_tokens = split_docs(df_train_clean)\n",
    "valid_tokens = split_docs(df_valid_clean)\n",
    "test_tokens = split_docs(df_test_clean)\n",
    "\n",
    "vocab, idx_train_tokens, word2idx = tokenize(train_tokens)\n",
    "_, idx_valid_tokens, _ = tokenize(valid_tokens)\n",
    "_, idx_test_tokens, _ = tokenize(test_tokens)\n",
    "\n",
    "pad_idx = word2idx['<PAD>']\n",
    "skipgrams_train = create_skipgrams(idx_train_tokens, window_size=2, pad_idx=pad_idx)\n",
    "skipgrams_valid = create_skipgrams(idx_valid_tokens, window_size=2, pad_idx=pad_idx)\n",
    "skipgrams_test = create_skipgrams(idx_test_tokens, window_size=2, pad_idx=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23042, 3358115)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list = list(word2idx.keys())\n",
    "vocab_size = len(words_list)\n",
    "n = len(skipgrams_train)\n",
    "vocab_size, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6558"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = n // batch_size\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle skipgrams\n",
    "random.shuffle(skipgrams_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_batch = []\n",
    "target_batch = []\n",
    "negative_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 30, 1958, 5569] 336\n",
      "[9190, 1, 24, 30] 3176\n",
      "[1529, 701, 7251, 5616] 30\n",
      "[34, 2093, 43, 30] 88\n",
      "[29, 24, 1, 0] 11015\n",
      "[1143, 43, 2173, 11082] 30\n",
      "[267, 29, 0, 0] 853\n",
      "[1973, 244, 268, 1786] 276\n",
      "[2092, 16081, 16, 4636] 2180\n",
      "[235, 30, 8343, 348] 65\n"
     ]
    }
   ],
   "source": [
    "for contexts, target in skipgrams_train[:10]:\n",
    "    print(contexts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(vocab, target, num_samples):\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        sample = random.choice(vocab)\n",
    "        if sample != target:\n",
    "            negative_samples.append(sample)\n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in num_batches:\n",
    "    start_idx = batch * batch_size\n",
    "    end_idx = min((batch + 1) * batch_size, n)\n",
    "    batch = skipgrams_train[start_idx:end_idx]\n",
    "    context, target = zip(*batch)\n",
    "    context_batch.append(context)\n",
    "    target_batch.append(target)\n",
    "    negative_batch.append(get_negative_samples(target, vocab_size, num_neg_samples=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = list(word2idx.keys())\n",
    "vocab_size = len(words_list)\n",
    "n = len(skipgrams)\n",
    "\n",
    "# Shuffle skipgrams\n",
    "random.shuffle(skipgrams)\n",
    "\n",
    "for batch_start in range(0, n, batch_size):\n",
    "    context_batch = []\n",
    "    target_batch = []\n",
    "    negative_batch = []\n",
    "    \n",
    "    # Create batches\n",
    "    for contexts, target in skipgrams[batch_start:batch_start + batch_size]:\n",
    "        negatives = [random.choice(range(vocab_size)) for _ in range(num_neg_samples)]\n",
    "        context_batch.append(torch.LongTensor(contexts))\n",
    "        target_batch.append(torch.LongTensor([target]))\n",
    "        negative_batch.append(torch.LongTensor(negatives))\n",
    "\n",
    "    # Pad context sequences in batch\n",
    "    context_batch = pad_sequence(context_batch, batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "    # Convert target and negative batches to tensors\n",
    "    target_batch = torch.stack(target_batch)\n",
    "    negative_batch = torch.stack(negative_batch)\n",
    "    \n",
    "    yield context_batch, target_batch, negative_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gen = data_generator(skipgrams_train, word2idx)\n",
    "# valid_gen = data_generator(skipgrams_valid, word2idx)\n",
    "# test_gen = data_generator(skipgrams_test, word2idx)\n",
    "\n",
    "train_data_gen = data_generator(skipgrams_train, word2idx, pad_idx=word2idx['<PAD>'], batch_size=512, num_neg_samples=5)\n",
    "valid_data_gen = data_generator(skipgrams_valid, word2idx, pad_idx=word2idx['<PAD>'], batch_size=512, num_neg_samples=5)\n",
    "test_data_gen = data_generator(skipgrams_test, word2idx, pad_idx=word2idx['<PAD>'], batch_size=512, num_neg_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_NS(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW_NS, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, context_words, target_word, negative_words):\n",
    "        # Get embeddings for context words, target word and negative words\n",
    "        context_embeds = self.embeddings(context_words)  # (batch_size, window_size*2, embed_size)\n",
    "        target_embeds = self.embeddings(target_word)    # (batch_size, 1, embed_size)\n",
    "        negative_embeds = self.embeddings(negative_words)# (batch_size, num_neg_samples, embed_size)\n",
    "\n",
    "        # Sum the context word embeddings\n",
    "        context_embeds_sum = torch.sum(context_embeds, dim=1, keepdim=True)  # (batch_size, 1, embed_size)\n",
    "\n",
    "        # Compute positive score\n",
    "        pos_score = torch.bmm(context_embeds_sum, target_embeds.transpose(1,2)) # (batch_size, 1, 1)\n",
    "        pos_score = F.logsigmoid(pos_score)\n",
    "\n",
    "        # Compute negative score\n",
    "        neg_score = torch.bmm(context_embeds_sum, negative_embeds.transpose(1,2)) # (batch_size, 1, num_neg_samples)\n",
    "        neg_score = F.logsigmoid(-neg_score)\n",
    "\n",
    "        # Return negative of total score\n",
    "        return -(torch.sum(pos_score) + torch.sum(neg_score))\n",
    "    \n",
    "    \n",
    "def train(model, epochs, data_generator, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for context_batch, target_batch, negative_batch in data_generator:\n",
    "            model.zero_grad()\n",
    "            loss = model(context_batch, target_batch, negative_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / batch_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 10949.492521160095\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-006.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-006.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embed_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-006.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m CBOW_NS(vocab_size, embed_size)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-006.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(model, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, data_generator\u001b[39m=\u001b[39;49mtrain_data_gen)\n",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-006.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, data_generator, lr)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-006.ipynb#X26sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-006.ipynb#X26sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     batch_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-006.ipynb#X26sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss \u001b[39m/\u001b[39m batch_count\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 100\n",
    "model = CBOW_NS(vocab_size, embed_size)\n",
    "train(model, epochs=3, data_generator=train_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary variable to hold the batch count\n",
    "temp_batch_count = 0\n",
    "\n",
    "# Manually iterate through your data generator\n",
    "for batch in train_data_gen:\n",
    "    temp_batch_count += 1\n",
    "\n",
    "# Check how many batches were created\n",
    "print(f'Number of batches: {temp_batch_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_percentile_length(list_of_lists, percentile):\n",
    "    # Step 1: Calculate the lengths of all sublists\n",
    "    lengths = [len(sublist) for sublist in list_of_lists]\n",
    "    \n",
    "    # Step 2: Sort the lengths in ascending order\n",
    "    sorted_lengths = sorted(lengths)\n",
    "    \n",
    "    # Step 3: Find the index of the percentile in the sorted lengths\n",
    "    index = (percentile / 100) * (len(sorted_lengths) - 1)\n",
    "    \n",
    "    # Step 4: Check if the index is an integer or not\n",
    "    if index.is_integer():\n",
    "        # If the index is an integer, return the corresponding value\n",
    "        percentile_length = sorted_lengths[int(index)]\n",
    "    else:\n",
    "        # If the index is not an integer, interpolate between two values\n",
    "        lower_index = math.floor(index)\n",
    "        upper_index = math.ceil(index)\n",
    "        lower_value = sorted_lengths[lower_index]\n",
    "        upper_value = sorted_lengths[upper_index]\n",
    "        percentile_length = np.interp(index, [lower_index, upper_index], [lower_value, upper_value])\n",
    "    \n",
    "    return percentile_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_75th = find_percentile_length(idx_train_tokens, 75)\n",
    "percentile_90th = find_percentile_length(idx_train_tokens, 90)\n",
    "\n",
    "print(\"75th percentile length of lists:\", percentile_75th)\n",
    "print(\"90th percentile length of lists:\", percentile_90th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, epochs=3, data_generator=train_gen, batch_size=1000, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(word2idx)\n",
    "# embed_size = 100  # dimension of the embedding vectors\n",
    "# cbow = CBOW(vocab_size, embed_size)\n",
    "# data_generator = generate_batches(skipgrams, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
