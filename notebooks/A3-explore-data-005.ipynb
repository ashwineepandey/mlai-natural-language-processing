{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Union\n",
    "import log\n",
    "import mynlputils as nu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "# from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nu.timer\n",
    "def load_data(raw_txt_train_path: str, raw_txt_test_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads the training and test data.\n",
    "    Args:\n",
    "        raw_txt_train_path (str): Path to the training data file.\n",
    "        raw_txt_test_path (str): Path to the test data file.\n",
    "    Returns:\n",
    "        train_data (DataFrame): Training data.\n",
    "        test_data (DataFrame): Test data.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(raw_txt_train_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    df_test = pd.read_csv(raw_txt_test_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    logger.info(f\"df_train.shape: {df_train.shape}\")\n",
    "    logger.info(f\"df_train unique Titles: {df_train['title'].nunique()}\")\n",
    "    logger.info(f\"df_train unique Labels: {df_train['label'].value_counts()}\")\n",
    "    logger.info(f\"df_test.shape: {df_test.shape}\")\n",
    "    logger.info(f\"df_test unique Titles: {df_test['title'].nunique()}\")\n",
    "    logger.info(f\"df_test unique Labels: {df_test['label'].value_counts()}\")\n",
    "    return df_train[[\"label\", \"description\"]], df_test[[\"label\", \"description\"]]\n",
    "\n",
    "@nu.timer\n",
    "def create_validation_set(corpus: pd.DataFrame, valid_size: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates a validation set from a list of DataFrames each representing a sentence.\n",
    "\n",
    "    Args:\n",
    "    corpus (pd.DataFrame): List of DataFrames each representing a sentence.\n",
    "    valid_size (float): Proportion of sentences to include in the validation set.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: Training and validation sets.\n",
    "    \"\"\"\n",
    "    train_corpus, valid_corpus = train_test_split(corpus, test_size=valid_size, random_state=1)\n",
    "    logger.info(f\"train_corpus.shape: {train_corpus.shape}\")\n",
    "    logger.info(f\"train_corpus unique Labels: {train_corpus['label'].value_counts()}\")\n",
    "    logger.info(f\"valid_corpus.shape: {valid_corpus.shape}\")\n",
    "    logger.info(f\"valid_corpus unique Labels: {valid_corpus['label'].value_counts()}\")\n",
    "    return train_corpus.reset_index(drop=True), valid_corpus.reset_index(drop=True)\n",
    "\n",
    "@nu.timer\n",
    "def clean_text(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    clean_docs = docs['description']\n",
    "    clean_docs = clean_docs.str.replace(\"-\", \" \") # Separate hyphenated words\n",
    "    clean_docs = clean_docs.str.replace(\"quot;\", \" \") # Remove HTML encoding for \"\n",
    "    clean_docs = clean_docs.str.replace(\"#39;s\", \"'\") # Remove HTML encoding for 's\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "    clean_docs = clean_docs.str.translate(translation_table)\n",
    "    clean_docs = clean_docs.str.lower() # Lowercase the text\n",
    "    clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ') # Replace digits with <NUM>\n",
    "    clean_docs = clean_docs.str.replace(r'\\s+', ' ') # Replace multiple spaces with a single space\n",
    "    return clean_docs.to_frame()\n",
    "\n",
    "@nu.timer\n",
    "def split_docs(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    return docs['description'].str.split().to_list()\n",
    "\n",
    "@nu.timer\n",
    "def tokenize(tokens: List[List[str]], min_freq: int = 5):\n",
    "    word_freq = Counter([word for sentence in tokens for word in sentence])\n",
    "    vocab = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
    "    # Add <PAD>, <UNK> in the vocab\n",
    "    vocab = ['<PAD>', '<UNK>'] + vocab\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_tokens = [[word2idx.get(word, 1) for word in sentence] for sentence in tokens]  # 1 is the index of <UNK>\n",
    "    logger.info(f\"Vocab size: {len(vocab)}\")\n",
    "    return vocab, idx_tokens, word2idx\n",
    "\n",
    "\n",
    "@nu.timer\n",
    "def create_skipgrams(corpus, window_size, pad_idx):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        # Pad the sentence with window_size <PAD> tokens at the beginning and end\n",
    "        padded_sentence = [pad_idx] * window_size + sentence + [pad_idx] * window_size\n",
    "        for word_index in range(window_size, len(padded_sentence) - window_size):\n",
    "            # Select the context words from the padded sentence, excluding the current word\n",
    "            contexts = padded_sentence[word_index - window_size : word_index] + padded_sentence[word_index + 1 : word_index + window_size + 1]\n",
    "            # Add context and target pairs to the data\n",
    "            data.append((contexts, padded_sentence[word_index]))\n",
    "    logger.info(f\"Number of skipgrams: {len(data)}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_generator(skipgrams, word2idx, num_neg_samples=5):\n",
    "    words_list = list(word2idx.keys())\n",
    "    vocab_size = len(words_list)\n",
    "\n",
    "    for contexts, target in skipgrams:\n",
    "        # Generate negative samples for each context-target pair\n",
    "        negatives = [random.choice(range(vocab_size)) for _ in range(num_neg_samples)]\n",
    "        \n",
    "        # Ensure the contexts, target and negatives are lists of integers\n",
    "        if not all(isinstance(c, int) for c in contexts):\n",
    "            raise ValueError(f\"Contexts should be a list of integers. Got {contexts}\")\n",
    "        if not isinstance(target, int):\n",
    "            raise ValueError(f\"Target should be an integer. Got {target}\")\n",
    "        if not all(isinstance(n, int) for n in negatives):\n",
    "            raise ValueError(f\"Negatives should be a list of integers. Got {negatives}\")\n",
    "\n",
    "        yield torch.LongTensor([contexts]), torch.LongTensor([target]), torch.LongTensor(negatives)\n",
    "\n",
    "\n",
    "class CBOW_NS(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW_NS, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "    \n",
    "    def forward(self, target_word, context_words, negative_words):\n",
    "        # Reshape tensors to match dimensions expected by bmm\n",
    "        target_embeds = self.embeddings(target_word).unsqueeze(2)  # shape: (batch_size, embedding_dim, 1)\n",
    "        context_embeds = self.embeddings(context_words).unsqueeze(1)  # shape: (batch_size, 1, embedding_dim)\n",
    "        negative_embeds = self.embeddings(negative_words).unsqueeze(1)  # shape: (batch_size, 1, num_negative_samples)\n",
    "\n",
    "        # Calculate the positive log likelihood\n",
    "        pos_score = F.logsigmoid(torch.bmm(context_embeds, target_embeds).squeeze())\n",
    "\n",
    "        # Calculate the negative log likelihood\n",
    "        neg_score = F.logsigmoid(-1 * torch.bmm(negative_embeds, target_embeds.repeat(1, negative_embeds.size(2), 1)).squeeze())\n",
    "\n",
    "        # Sum up and return negative of scores (as we want to minimize negative log likelihood)\n",
    "        return -(torch.sum(pos_score) + torch.sum(neg_score))\n",
    "\n",
    "\n",
    "def train(model, epochs, data_generator, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for target, context, negative in data_generator:\n",
    "            target = torch.LongTensor([target])\n",
    "            context = torch.LongTensor([context])\n",
    "            negative = torch.LongTensor(negative)\n",
    "            model.zero_grad()\n",
    "            loss = model(target, context, negative)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        logger.info(f'Loss at epoch {epoch}: {total_loss/len(skipgrams)}')\n",
    "    \n",
    "\n",
    "# def train(model, epochs, data_generator, lr=0.001):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "#         for target, context, negative in data_generator:\n",
    "#             model.zero_grad()\n",
    "#             loss = model(target, context, negative)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#         logger.info(f'Loss at epoch {epoch}: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-Jul-23 11:10:11 - INFO - Starting 'load_config'.\n",
      "23-Jul-23 11:10:11 - INFO - Finished 'load_config' in 0.0070 secs.\n",
      "23-Jul-23 11:10:11 - INFO - Starting 'load_data'.\n",
      "23-Jul-23 11:10:11 - INFO - df_train.shape: (120000, 3)\n",
      "23-Jul-23 11:10:11 - INFO - df_train unique Titles: 114364\n",
      "23-Jul-23 11:10:11 - INFO - df_train unique Labels: 3    30000\n",
      "4    30000\n",
      "2    30000\n",
      "1    30000\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 11:10:11 - INFO - df_test.shape: (7600, 3)\n",
      "23-Jul-23 11:10:11 - INFO - df_test unique Titles: 7569\n",
      "23-Jul-23 11:10:11 - INFO - df_test unique Labels: 3    1900\n",
      "4    1900\n",
      "2    1900\n",
      "1    1900\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 11:10:11 - INFO - Finished 'load_data' in 0.2931 secs.\n",
      "23-Jul-23 11:10:11 - INFO - Starting 'create_validation_set'.\n",
      "23-Jul-23 11:10:11 - INFO - train_corpus.shape: (108000, 2)\n",
      "23-Jul-23 11:10:11 - INFO - train_corpus unique Labels: 3    27024\n",
      "4    27012\n",
      "1    26982\n",
      "2    26982\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 11:10:11 - INFO - valid_corpus.shape: (12000, 2)\n",
      "23-Jul-23 11:10:11 - INFO - valid_corpus unique Labels: 1    3018\n",
      "2    3018\n",
      "4    2988\n",
      "3    2976\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 11:10:11 - INFO - Finished 'create_validation_set' in 0.0089 secs.\n",
      "23-Jul-23 11:10:11 - INFO - Starting 'clean_text'.\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_66489/4179282293.py:50: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ') # Replace digits with <NUM>\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_66489/4179282293.py:51: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\s+', ' ') # Replace multiple spaces with a single space\n",
      "23-Jul-23 11:10:12 - INFO - Finished 'clean_text' in 1.3719 secs.\n",
      "23-Jul-23 11:10:12 - INFO - Starting 'split_docs'.\n",
      "23-Jul-23 11:10:13 - INFO - Finished 'split_docs' in 0.3413 secs.\n",
      "23-Jul-23 11:10:13 - INFO - Starting 'tokenize'.\n",
      "23-Jul-23 11:10:13 - INFO - Vocab size: 23042\n",
      "23-Jul-23 11:10:13 - INFO - Finished 'tokenize' in 0.6904 secs.\n",
      "23-Jul-23 11:10:13 - INFO - Starting 'create_skipgrams'.\n",
      "23-Jul-23 11:10:17 - INFO - Number of skipgrams: 3358115\n",
      "23-Jul-23 11:10:17 - INFO - Finished 'create_skipgrams' in 3.1962 secs.\n"
     ]
    }
   ],
   "source": [
    "conf = nu.load_config(\"a3\")\n",
    "df_train, df_test = load_data(conf.paths.raw_txt_train, conf.paths.raw_txt_test)\n",
    "train_corpus, valid_corpus = create_validation_set(corpus = df_train, valid_size = 0.1)\n",
    "clean_docs = clean_text(train_corpus)\n",
    "raw_tokens = split_docs(clean_docs)\n",
    "vocab, idx_tokens, word2idx = tokenize(raw_tokens)\n",
    "skipgrams = create_skipgrams(idx_tokens, window_size=2, pad_idx=word2idx['<PAD>'])\n",
    "window_size = 2  # Assume a suitable window size\n",
    "num_negatives = 5  # Assume a suitable number of negative samples\n",
    "data_gen = data_generator(skipgrams, word2idx)\n",
    "# Create and train the model\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 100\n",
    "model = CBOW_NS(vocab_size, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batch2 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, data_generator\u001b[39m=\u001b[39;49mdata_gen)\n",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb Cell 5\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, data_generator, lr)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mfor\u001b[39;00m target, context, negative \u001b[39min\u001b[39;00m data_generator:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m     model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     loss \u001b[39m=\u001b[39m model(target, context, negative)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlai_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb Cell 5\u001b[0m in \u001b[0;36mCBOW_NS.forward\u001b[0;34m(self, target_word, context_words, negative_words)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m negative_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(negative_words)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)  \u001b[39m# shape: (batch_size, 1, num_negative_samples)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39m# Calculate the positive log likelihood\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m pos_score \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlogsigmoid(torch\u001b[39m.\u001b[39;49mbmm(context_embeds, target_embeds)\u001b[39m.\u001b[39msqueeze())\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m# Calculate the negative log likelihood\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ashwineekumarpandey/Documents/Development/MLAI/GitHub/mlai-natural-language-processing/notebooks/A3-explore-data-005.ipynb#X13sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m neg_score \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlogsigmoid(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mbmm(negative_embeds, target_embeds\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, negative_embeds\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m), \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batch2 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "train(model, epochs=10, data_generator=data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(word2idx)\n",
    "# embed_size = 100  # dimension of the embedding vectors\n",
    "# cbow = CBOW(vocab_size, embed_size)\n",
    "# data_generator = generate_batches(skipgrams, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
