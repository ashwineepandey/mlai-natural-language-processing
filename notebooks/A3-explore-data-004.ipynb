{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Union\n",
    "import log\n",
    "import mynlputils as nu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "# from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nu.timer\n",
    "def load_data(raw_txt_train_path: str, raw_txt_test_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads the training and test data.\n",
    "    Args:\n",
    "        raw_txt_train_path (str): Path to the training data file.\n",
    "        raw_txt_test_path (str): Path to the test data file.\n",
    "    Returns:\n",
    "        train_data (DataFrame): Training data.\n",
    "        test_data (DataFrame): Test data.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(raw_txt_train_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    df_test = pd.read_csv(raw_txt_test_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    logger.info(f\"df_train.shape: {df_train.shape}\")\n",
    "    logger.info(f\"df_train unique Titles: {df_train['title'].nunique()}\")\n",
    "    logger.info(f\"df_train unique Labels: {df_train['label'].value_counts()}\")\n",
    "    logger.info(f\"df_test.shape: {df_test.shape}\")\n",
    "    logger.info(f\"df_test unique Titles: {df_test['title'].nunique()}\")\n",
    "    logger.info(f\"df_test unique Labels: {df_test['label'].value_counts()}\")\n",
    "    return df_train[[\"label\", \"description\"]], df_test[[\"label\", \"description\"]]\n",
    "\n",
    "@nu.timer\n",
    "def create_validation_set(corpus: pd.DataFrame, valid_size: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates a validation set from a list of DataFrames each representing a sentence.\n",
    "\n",
    "    Args:\n",
    "    corpus (pd.DataFrame): List of DataFrames each representing a sentence.\n",
    "    valid_size (float): Proportion of sentences to include in the validation set.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: Training and validation sets.\n",
    "    \"\"\"\n",
    "    train_corpus, valid_corpus = train_test_split(corpus, test_size=valid_size, random_state=1)\n",
    "    logger.info(f\"train_corpus.shape: {train_corpus.shape}\")\n",
    "    logger.info(f\"train_corpus unique Labels: {train_corpus['label'].value_counts()}\")\n",
    "    logger.info(f\"valid_corpus.shape: {valid_corpus.shape}\")\n",
    "    logger.info(f\"valid_corpus unique Labels: {valid_corpus['label'].value_counts()}\")\n",
    "    return train_corpus.reset_index(drop=True), valid_corpus.reset_index(drop=True)\n",
    "\n",
    "@nu.timer\n",
    "def clean_text(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    clean_docs = docs['description']\n",
    "    clean_docs = clean_docs.str.replace(\"-\", \" \") # Separate hyphenated words\n",
    "    clean_docs = clean_docs.str.replace(\"quot;\", \" \") # Remove HTML encoding for \"\n",
    "    clean_docs = clean_docs.str.replace(\"#39;s\", \"'\") # Remove HTML encoding for 's\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "    clean_docs = clean_docs.str.translate(translation_table)\n",
    "    clean_docs = clean_docs.str.lower() # Lowercase the text\n",
    "    clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ') # Replace digits with <NUM>\n",
    "    clean_docs = clean_docs.str.replace(r'\\s+', ' ') # Replace multiple spaces with a single space\n",
    "    return clean_docs.to_frame()\n",
    "\n",
    "@nu.timer\n",
    "def split_docs(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    return docs['description'].str.split().to_list()\n",
    "\n",
    "@nu.timer\n",
    "def tokenize(tokens: List[List[str]], min_freq: int = 5):\n",
    "    word_freq = Counter([word for sentence in tokens for word in sentence])\n",
    "    vocab = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
    "    # Add <PAD>, <UNK> in the vocab\n",
    "    vocab = ['<PAD>', '<UNK>'] + vocab\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_tokens = [[word2idx.get(word, 1) for word in sentence] for sentence in tokens]  # 1 is the index of <UNK>\n",
    "    logger.info(f\"Vocab size: {len(vocab)}\")\n",
    "    return vocab, idx_tokens, word2idx\n",
    "\n",
    "# def tokenize(tokens: List[List[str]], min_freq: int = 5):\n",
    "#     word_freq = Counter([word for sentence in tokens for word in sentence])\n",
    "#     vocab = [word if word_freq[word] >= min_freq else '<UNK>' for word in word_freq]\n",
    "#     # Add <PAD>, <UNK> in the vocab\n",
    "#     vocab.insert(0, '<PAD>')\n",
    "#     word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "#     tokens = [[word if word in word2idx else '<UNK>' for word in sentence] for sentence in tokens]\n",
    "#     idx_tokens = [[word2idx[word] for word in sentence] for sentence in tokens]\n",
    "#     logger.info(f\"Vocab size: {len(vocab)}\")\n",
    "#     return vocab, idx_tokens, word2idx\n",
    "\n",
    "@nu.timer\n",
    "def create_skipgrams(corpus, window_size, pad_idx):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        # Pad the sentence with window_size <PAD> tokens at the beginning and end\n",
    "        padded_sentence = [pad_idx] * window_size + sentence + [pad_idx] * window_size\n",
    "        for word_index in range(window_size, len(padded_sentence) - window_size):\n",
    "            # Select the context words from the padded sentence, excluding the current word\n",
    "            contexts = padded_sentence[word_index - window_size : word_index] + padded_sentence[word_index + 1 : word_index + window_size + 1]\n",
    "            data.append((contexts, padded_sentence[word_index]))\n",
    "    # For each target-context pair, select num_negatives random words from the vocabulary.\n",
    "    negative_samples = np.random.choice(vocab, size=(len(skipgrams), num_negatives))\n",
    "    # Add the negative samples to the skipgrams\n",
    "    skipgrams = [(target, context, negative_samples[i]) for i, (target, context) in enumerate(skipgrams)]\n",
    "    logger.info(f\"Number of skipgrams: {len(data)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-Jul-23 00:45:08 - INFO - Starting 'load_config'.\n",
      "23-Jul-23 00:45:08 - INFO - Finished 'load_config' in 0.0077 secs.\n",
      "23-Jul-23 00:45:08 - INFO - Starting 'load_data'.\n",
      "23-Jul-23 00:45:08 - INFO - df_train.shape: (120000, 3)\n",
      "23-Jul-23 00:45:08 - INFO - df_train unique Titles: 114364\n",
      "23-Jul-23 00:45:08 - INFO - df_train unique Labels: 3    30000\n",
      "4    30000\n",
      "2    30000\n",
      "1    30000\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 00:45:08 - INFO - df_test.shape: (7600, 3)\n",
      "23-Jul-23 00:45:08 - INFO - df_test unique Titles: 7569\n",
      "23-Jul-23 00:45:08 - INFO - df_test unique Labels: 3    1900\n",
      "4    1900\n",
      "2    1900\n",
      "1    1900\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 00:45:08 - INFO - Finished 'load_data' in 0.2949 secs.\n",
      "23-Jul-23 00:45:08 - INFO - Starting 'create_validation_set'.\n",
      "23-Jul-23 00:45:08 - INFO - train_corpus.shape: (108000, 2)\n",
      "23-Jul-23 00:45:08 - INFO - train_corpus unique Labels: 3    27024\n",
      "4    27012\n",
      "1    26982\n",
      "2    26982\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 00:45:08 - INFO - valid_corpus.shape: (12000, 2)\n",
      "23-Jul-23 00:45:08 - INFO - valid_corpus unique Labels: 1    3018\n",
      "2    3018\n",
      "4    2988\n",
      "3    2976\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 00:45:08 - INFO - Finished 'create_validation_set' in 0.0099 secs.\n",
      "23-Jul-23 00:45:08 - INFO - Starting 'clean_text'.\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_51148/2980891732.py:50: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ') # Replace digits with <NUM>\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_51148/2980891732.py:51: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\s+', ' ') # Replace multiple spaces with a single space\n",
      "23-Jul-23 00:45:10 - INFO - Finished 'clean_text' in 1.3883 secs.\n",
      "23-Jul-23 00:45:10 - INFO - Starting 'split_docs'.\n",
      "23-Jul-23 00:45:10 - INFO - Finished 'split_docs' in 0.3795 secs.\n",
      "23-Jul-23 00:45:10 - INFO - Starting 'tokenize'.\n",
      "23-Jul-23 00:45:11 - INFO - Vocab size: 23042\n",
      "23-Jul-23 00:45:11 - INFO - Finished 'tokenize' in 0.7136 secs.\n",
      "23-Jul-23 00:45:11 - INFO - Starting 'create_skipgrams'.\n",
      "23-Jul-23 00:45:14 - INFO - Number of skipgrams: 3358115\n",
      "23-Jul-23 00:45:14 - INFO - Finished 'create_skipgrams' in 3.2607 secs.\n"
     ]
    }
   ],
   "source": [
    "conf = nu.load_config(\"a3\")\n",
    "df_train, df_test = load_data(conf.paths.raw_txt_train, conf.paths.raw_txt_test)\n",
    "train_corpus, valid_corpus = create_validation_set(corpus = df_train, valid_size = 0.1)\n",
    "clean_docs = clean_text(train_corpus)\n",
    "raw_tokens = split_docs(clean_docs)\n",
    "vocab, idx_tokens, word2idx = tokenize(raw_tokens)\n",
    "skipgrams = create_skipgrams(idx_tokens, window_size=2, pad_idx=word2idx['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_NS(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW_NS, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "    \n",
    "    def forward(self, target_word, context_words, negative_words):\n",
    "        target_embeds = self.embeddings(target_word)\n",
    "        context_embeds = self.embeddings(context_words)\n",
    "        negative_embeds = self.embeddings(negative_words)\n",
    "\n",
    "        # Calculate the positive log likelihood\n",
    "        pos_score = F.logsigmoid(torch.bmm(target_embeds, context_embeds.transpose(1,2)).squeeze())\n",
    "        \n",
    "        # Calculate the negative log likelihood\n",
    "        neg_score = F.logsigmoid(-1 * torch.bmm(target_embeds, negative_embeds.transpose(1,2)).squeeze())\n",
    "        \n",
    "        return -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
    "\n",
    "def train(model, epochs, data_generator, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for target, context, negative in data_generator:\n",
    "            target = torch.LongTensor([target])\n",
    "            context = torch.LongTensor([context])\n",
    "            negative = torch.LongTensor(negative)\n",
    "            model.zero_grad()\n",
    "            loss = model(target, context, negative)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        logger.info(f'Loss at epoch {epoch}: {total_loss/len(skipgrams)}')\n",
    "    \n",
    "\n",
    "def data_generator(skip_grams, word_counts, t=1e-5, neg_samples=5):\n",
    "    total_counts = sum(word_counts.values())\n",
    "    word_freqs = {word: count/total_counts for word, count in word_counts.items()}\n",
    "    word_probs = {word: 1 - np.sqrt(t / freq) for word, freq in word_freqs.items()}\n",
    "\n",
    "    # calculate negative sampling probabilities\n",
    "    word_counts_pow = np.array(list(word_counts.values()))**0.75\n",
    "    neg_sampling_probs = word_counts_pow / np.sum(word_counts_pow)\n",
    "    words_list = list(word_counts.keys())\n",
    "\n",
    "    for skip_gram in skip_grams:\n",
    "        # subsample\n",
    "        if np.random.rand() < word_probs[skip_gram[0]]:\n",
    "            continue\n",
    "        \n",
    "        # create negative samples\n",
    "        negatives = np.random.choice(words_list, size=neg_samples, p=neg_sampling_probs)\n",
    "\n",
    "        yield (skip_gram[0], skip_gram[1], negatives)\n",
    "\n",
    "\n",
    "def generate_batches(skipgrams, batch_size):\n",
    "    x, y = [], []\n",
    "    for i, (context, target) in enumerate(skipgrams, 1):\n",
    "        x.append(context)\n",
    "        y.append(target)\n",
    "        if i % batch_size == 0:\n",
    "            yield torch.LongTensor(x), torch.LongTensor(y)\n",
    "            x, y = [], []\n",
    "    if x and y:\n",
    "        yield torch.LongTensor(x), torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)\n",
    "embed_size = 100  # dimension of the embedding vectors\n",
    "cbow = CBOW(vocab_size, embed_size)\n",
    "data_generator = generate_batches(skipgrams, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-Jul-23 01:01:01 - INFO - Epoch: 0, Loss: 304738.6271183491\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 1, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 2, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 3, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 4, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 5, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 6, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 7, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 8, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 9, Loss: 0\n"
     ]
    }
   ],
   "source": [
    "train(cbow, epochs=10, data_generator=data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
