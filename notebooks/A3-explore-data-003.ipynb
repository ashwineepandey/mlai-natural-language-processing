{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Union\n",
    "import log\n",
    "import mynlputils as nu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "# from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nu.timer\n",
    "def load_data(raw_txt_train_path: str, raw_txt_test_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads the training and test data.\n",
    "    Args:\n",
    "        raw_txt_train_path (str): Path to the training data file.\n",
    "        raw_txt_test_path (str): Path to the test data file.\n",
    "    Returns:\n",
    "        train_data (DataFrame): Training data.\n",
    "        test_data (DataFrame): Test data.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(raw_txt_train_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    df_test = pd.read_csv(raw_txt_test_path, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "    logger.info(f\"df_train.shape: {df_train.shape}\")\n",
    "    logger.info(f\"df_train unique Titles: {df_train['title'].nunique()}\")\n",
    "    logger.info(f\"df_train unique Labels: {df_train['label'].value_counts()}\")\n",
    "    logger.info(f\"df_test.shape: {df_test.shape}\")\n",
    "    logger.info(f\"df_test unique Titles: {df_test['title'].nunique()}\")\n",
    "    logger.info(f\"df_test unique Labels: {df_test['label'].value_counts()}\")\n",
    "    return df_train[[\"label\", \"description\"]], df_test[[\"label\", \"description\"]]\n",
    "\n",
    "@nu.timer\n",
    "def create_validation_set(corpus: pd.DataFrame, valid_size: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates a validation set from a list of DataFrames each representing a sentence.\n",
    "\n",
    "    Args:\n",
    "    corpus (pd.DataFrame): List of DataFrames each representing a sentence.\n",
    "    valid_size (float): Proportion of sentences to include in the validation set.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: Training and validation sets.\n",
    "    \"\"\"\n",
    "    train_corpus, valid_corpus = train_test_split(corpus, test_size=valid_size, random_state=1)\n",
    "    logger.info(f\"train_corpus.shape: {train_corpus.shape}\")\n",
    "    logger.info(f\"train_corpus unique Labels: {train_corpus['label'].value_counts()}\")\n",
    "    logger.info(f\"valid_corpus.shape: {valid_corpus.shape}\")\n",
    "    logger.info(f\"valid_corpus unique Labels: {valid_corpus['label'].value_counts()}\")\n",
    "    return train_corpus.reset_index(drop=True), valid_corpus.reset_index(drop=True)\n",
    "\n",
    "@nu.timer\n",
    "def clean_text(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    clean_docs = docs['description']\n",
    "    clean_docs = clean_docs.str.replace(\"-\", \" \") # Separate hyphenated words\n",
    "    clean_docs = clean_docs.str.replace(\"quot;\", \" \") # Remove HTML encoding for \"\n",
    "    clean_docs = clean_docs.str.replace(\"#39;s\", \"'\") # Remove HTML encoding for 's\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "    clean_docs = clean_docs.str.translate(translation_table)\n",
    "    clean_docs = clean_docs.str.lower() # Lowercase the text\n",
    "    clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ') # Replace digits with <NUM>\n",
    "    clean_docs = clean_docs.str.replace(r'\\s+', ' ') # Replace multiple spaces with a single space\n",
    "    return clean_docs.to_frame()\n",
    "\n",
    "@nu.timer\n",
    "def split_docs(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    return docs['description'].str.split().to_list()\n",
    "\n",
    "@nu.timer\n",
    "def tokenize(tokens: List[List[str]], min_freq: int = 5):\n",
    "    word_freq = Counter([word for sentence in tokens for word in sentence])\n",
    "    vocab = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
    "    # Add <PAD>, <UNK> in the vocab\n",
    "    vocab = ['<PAD>', '<UNK>'] + vocab\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_tokens = [[word2idx.get(word, 1) for word in sentence] for sentence in tokens]  # 1 is the index of <UNK>\n",
    "    logger.info(f\"Vocab size: {len(vocab)}\")\n",
    "    return vocab, idx_tokens, word2idx\n",
    "\n",
    "# def tokenize(tokens: List[List[str]], min_freq: int = 5):\n",
    "#     word_freq = Counter([word for sentence in tokens for word in sentence])\n",
    "#     vocab = [word if word_freq[word] >= min_freq else '<UNK>' for word in word_freq]\n",
    "#     # Add <PAD>, <UNK> in the vocab\n",
    "#     vocab.insert(0, '<PAD>')\n",
    "#     word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "#     tokens = [[word if word in word2idx else '<UNK>' for word in sentence] for sentence in tokens]\n",
    "#     idx_tokens = [[word2idx[word] for word in sentence] for sentence in tokens]\n",
    "#     logger.info(f\"Vocab size: {len(vocab)}\")\n",
    "#     return vocab, idx_tokens, word2idx\n",
    "\n",
    "@nu.timer\n",
    "def create_skipgrams(corpus, window_size, pad_idx):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        # Pad the sentence with window_size <PAD> tokens at the beginning and end\n",
    "        padded_sentence = [pad_idx] * window_size + sentence + [pad_idx] * window_size\n",
    "        for word_index in range(window_size, len(padded_sentence) - window_size):\n",
    "            # Select the context words from the padded sentence, excluding the current word\n",
    "            contexts = padded_sentence[word_index - window_size : word_index] + padded_sentence[word_index + 1 : word_index + window_size + 1]\n",
    "            data.append((contexts, padded_sentence[word_index]))\n",
    "    logger.info(f\"Number of skipgrams: {len(data)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-Jul-23 00:45:08 - INFO - Starting 'load_config'.\n",
      "23-Jul-23 00:45:08 - INFO - Finished 'load_config' in 0.0077 secs.\n",
      "23-Jul-23 00:45:08 - INFO - Starting 'load_data'.\n",
      "23-Jul-23 00:45:08 - INFO - df_train.shape: (120000, 3)\n",
      "23-Jul-23 00:45:08 - INFO - df_train unique Titles: 114364\n",
      "23-Jul-23 00:45:08 - INFO - df_train unique Labels: 3    30000\n",
      "4    30000\n",
      "2    30000\n",
      "1    30000\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 00:45:08 - INFO - df_test.shape: (7600, 3)\n",
      "23-Jul-23 00:45:08 - INFO - df_test unique Titles: 7569\n",
      "23-Jul-23 00:45:08 - INFO - df_test unique Labels: 3    1900\n",
      "4    1900\n",
      "2    1900\n",
      "1    1900\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 00:45:08 - INFO - Finished 'load_data' in 0.2949 secs.\n",
      "23-Jul-23 00:45:08 - INFO - Starting 'create_validation_set'.\n",
      "23-Jul-23 00:45:08 - INFO - train_corpus.shape: (108000, 2)\n",
      "23-Jul-23 00:45:08 - INFO - train_corpus unique Labels: 3    27024\n",
      "4    27012\n",
      "1    26982\n",
      "2    26982\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 00:45:08 - INFO - valid_corpus.shape: (12000, 2)\n",
      "23-Jul-23 00:45:08 - INFO - valid_corpus unique Labels: 1    3018\n",
      "2    3018\n",
      "4    2988\n",
      "3    2976\n",
      "Name: label, dtype: int64\n",
      "23-Jul-23 00:45:08 - INFO - Finished 'create_validation_set' in 0.0099 secs.\n",
      "23-Jul-23 00:45:08 - INFO - Starting 'clean_text'.\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_51148/2980891732.py:50: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\d+', ' <NUM> ') # Replace digits with <NUM>\n",
      "/var/folders/0g/blggksdj42z52nv3fy0h5b880000gn/T/ipykernel_51148/2980891732.py:51: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_docs = clean_docs.str.replace(r'\\s+', ' ') # Replace multiple spaces with a single space\n",
      "23-Jul-23 00:45:10 - INFO - Finished 'clean_text' in 1.3883 secs.\n",
      "23-Jul-23 00:45:10 - INFO - Starting 'split_docs'.\n",
      "23-Jul-23 00:45:10 - INFO - Finished 'split_docs' in 0.3795 secs.\n",
      "23-Jul-23 00:45:10 - INFO - Starting 'tokenize'.\n",
      "23-Jul-23 00:45:11 - INFO - Vocab size: 23042\n",
      "23-Jul-23 00:45:11 - INFO - Finished 'tokenize' in 0.7136 secs.\n",
      "23-Jul-23 00:45:11 - INFO - Starting 'create_skipgrams'.\n",
      "23-Jul-23 00:45:14 - INFO - Number of skipgrams: 3358115\n",
      "23-Jul-23 00:45:14 - INFO - Finished 'create_skipgrams' in 3.2607 secs.\n"
     ]
    }
   ],
   "source": [
    "conf = nu.load_config(\"a3\")\n",
    "df_train, df_test = load_data(conf.paths.raw_txt_train, conf.paths.raw_txt_test)\n",
    "train_corpus, valid_corpus = create_validation_set(corpus = df_train, valid_size = 0.1)\n",
    "clean_docs = clean_text(train_corpus)\n",
    "raw_tokens = split_docs(clean_docs)\n",
    "vocab, idx_tokens, word2idx = tokenize(raw_tokens)\n",
    "skipgrams = create_skipgrams(idx_tokens, window_size=2, pad_idx=word2idx['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        bow_embed = torch.mean(embeds, dim=1)\n",
    "        out = self.linear(bow_embed)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "\n",
    "def generate_batches(skipgrams, batch_size):\n",
    "    x, y = [], []\n",
    "    for i, (context, target) in enumerate(skipgrams, 1):\n",
    "        x.append(context)\n",
    "        y.append(target)\n",
    "        if i % batch_size == 0:\n",
    "            yield torch.LongTensor(x), torch.LongTensor(y)\n",
    "            x, y = [], []\n",
    "    if x and y:\n",
    "        yield torch.LongTensor(x), torch.LongTensor(y)\n",
    "\n",
    "\n",
    "def train(model, epochs, data_generator, lr=0.01):\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for context, target in data_generator:\n",
    "            context, target = Variable(context), Variable(target)\n",
    "            model.zero_grad()\n",
    "            log_probs = model(context)\n",
    "            loss = loss_fn(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        logger.info(f'Epoch: {epoch}, Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)\n",
    "embed_size = 100  # dimension of the embedding vectors\n",
    "cbow = CBOW(vocab_size, embed_size)\n",
    "data_generator = generate_batches(skipgrams, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-Jul-23 01:01:01 - INFO - Epoch: 0, Loss: 304738.6271183491\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 1, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 2, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 3, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 4, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 5, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 6, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 7, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 8, Loss: 0\n",
      "23-Jul-23 01:01:01 - INFO - Epoch: 9, Loss: 0\n"
     ]
    }
   ],
   "source": [
    "train(cbow, epochs=10, data_generator=data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
